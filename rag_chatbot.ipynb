{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPICGAGQLhL4"
      },
      "source": [
        "## Building RAG Chatbots with LangChain\n",
        "\n",
        "Using LangChain, OpenAI and Pinecone vector DB, I build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
        "\n",
        "I am using a dataset from a Llama2 ArXiv paper and related papers to provide context to the chatbot when it answers questions about the latest technologies and developments in Generative AI.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Python libraries used\n",
        "*   **langchain**: GenAI library, used to chain together different language components in the chatbot\n",
        "*   **openai**: OpenAI Python client\n",
        "*   **datasets**: Machine learning datasets, used as knowledge base for chatbot\n",
        "*   **pinecone-client**: Pinecone Python client, use Pinecone API to store chatbot's knowledge base in a vector database\n",
        "\n"
      ],
      "metadata": {
        "id": "zY0RF88KPf02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-31QBpOLhL8",
        "outputId": "86678983-748e-4013-ef4d-2bd8eec1ccb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.35)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain openai datasets tiktoken pinecone-client\n",
        "!pip install -qU langchain_openai\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When can prompt chatgpt using lists of messages. We can add new prompts by appending them to the existing list of messages."
      ],
      "metadata": {
        "id": "JeS647DgEwQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=openai.api_key,\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "jai7R_wURtJw"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpfu assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand string theory\")\n",
        "]\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)\n",
        "\n",
        "prompt = HumanMessage(content=\"Why do physicists believe it can produce a unified theory?\")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "print(res.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tbU9v2-war2",
        "outputId": "8de31dd6-7f86-4806-d17d-f8c6834f014b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String theory is a theoretical framework in physics that attempts to explain the fundamental particles and forces in the universe in terms of one-dimensional strings rather than point-like particles. These strings vibrate at different frequencies, giving rise to different particles and forces. The theory aims to unify general relativity (which describes gravity) and quantum mechanics (which describes the other fundamental forces) into a single framework.\n",
            "\n",
            "String theory is a complex and mathematically intricate subject that is still under active research and development. It has the potential to provide a unified description of all fundamental forces and particles in the universe, including gravity, electromagnetism, and the strong and weak nuclear forces. However, it also faces a number of challenges and criticisms, and many aspects of the theory remain speculative.\n",
            "\n",
            "If you have any specific questions about string theory or would like more information on a particular aspect of the theory, feel free to ask!\n",
            "Physicists believe that string theory has the potential to produce a unified theory because it aims to integrate the principles of general relativity (which describes the force of gravity) with those of quantum mechanics (which describe the behavior of particles on very small scales). By proposing that fundamental particles are not point-like objects, but rather tiny strings vibrating at different frequencies, string theory seeks to explain all fundamental forces and particles within a single framework. This could potentially lead to a unified understanding of the forces of nature and explain phenomena that are currently beyond the scope of existing theories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucinations\n",
        "\n",
        "The knowledge base of LLMs can be limited, because everything they know is learned during training. The LLM's worldview is entirely contained in the model's internal parameters. This is the chatbot's *parametric knowledge*.\n",
        "\n",
        "As a result, LLMs have no knowledge about new developments such as the Llama 2 LLM and LLMChain:"
      ],
      "metadata": {
        "id": "o1I2Dtb3FBGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "prompt = HumanMessage(\n",
        "    content=\"What is so special about Llama 2?\"\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN1iT5TZFpU_",
        "outputId": "7cbbe760-30ec-41e4-de8d-cea6324ea33f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I apologize but I'm not familiar with what \"Llama 2\" refers to. Can you provide more context or clarify your question so I can help you better?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "prompt = HumanMessage(\n",
        "    content=\"Tell me about LLMChain in LangChain?\"\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SwAEdBuItYu",
        "outputId": "35976699-9b3e-4613-f704-9e17c4842004"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I'm not familiar with \"LLMChain\" or \"LangChain.\" It's possible that these terms are related to specific projects or technologies that I have not encountered before. If you can provide more information or context, I'll do my best to assist you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way of feeding knowledge in LLMs is using *source knowledge*, which is information that is fed into the LLM via the prompt. We can do this manually by adding a description from LangChain documentation"
      ],
      "metadata": {
        "id": "LjWKKWDSH782"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_info = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_info)\n",
        "\n",
        "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts: {source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\"\n",
        "\n",
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMd_78PhJJiF",
        "outputId": "8445510e-5423-4dec-df3c-3ba26c23b61d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLMChain is a fundamental component within the LangChain framework for developing applications powered by language models. It is the most common type of chain and consists of a PromptTemplate, a model (which can be either an LLM or a ChatModel), and an optional output parser. The LLMChain takes multiple input variables, utilizes the PromptTemplate to format them into a prompt, then passes that prompt to the model. Finally, it applies the OutputParser (if provided) to parse the output of the LLM into a final format. The LLMChain within LangChain plays a crucial role in leveraging language models to create powerful and differentiated applications that are data-aware and provide an agentic interaction with the environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the Data\n",
        "\n",
        "We will be using the Hugging Face Dataset library to load context data. The dataset, \"`jamescalam/llama-2-arxiv-papers`\", contains a collection of ArXiv papers which serve as the external knowledge base the chatbot. Every entry in the dataset is a sample paragraph from a paper."
      ],
      "metadata": {
        "id": "Ep0ZdEmDL6_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_aZsrOeafTi",
        "outputId": "7abd63fb-1acd-4aee-a268-77813dec4818"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'doi': '1102.0183', 'chunk-id': '0', 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but', 'id': '1102.0183', 'title': 'High-Performance Neural Networks for Visual Object Classification', 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.', 'source': 'http://arxiv.org/pdf/1102.0183', 'authors': ['Dan C. Cireşan', 'Ueli Meier', 'Jonathan Masci', 'Luca M. Gambardella', 'Jürgen Schmidhuber'], 'categories': ['cs.AI', 'cs.NE'], 'comment': '12 pages, 2 figures, 5 tables', 'journal_ref': None, 'primary_category': 'cs.AI', 'published': '20110201', 'updated': '20110201', 'references': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Knowledge Base\n",
        "\n",
        "Using Pinecone I set up an embedding model and vector database. I am using OpenAI's `text-embedding-ada-002` model for the embeddings with `dimension` set to `1536`."
      ],
      "metadata": {
        "id": "y18MDwIPbpgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "import time\n",
        "\n",
        "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone(pc_api_key)\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "index_name = 'llama-2-rag'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    # create index if it doesn't exist\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "      time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV7TBkzHbRKH",
        "outputId": "831ed488-d031-401c-e9df-c977a7596afc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By making a vector embedding for the dataset we can perform queries on semantic similarity rather than matching words. We use Pinecone DB since it uses approximate nearest neighbors to quickly query similar items"
      ],
      "metadata": {
        "id": "1MG02BSKl0k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from tqdm import tqdm\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
        "\n",
        "data = dataset.to_pandas()\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "  i_end = min(len(data), i+batch_size)\n",
        "  batch = data.iloc[i:i_end]\n",
        "\n",
        "  # generate unique ids for each chunk\n",
        "  ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "  # embed text\n",
        "  texts = [x['chunk'] for _, x in batch.iterrows()]\n",
        "  embeds = embed_model.embed_documents(texts)\n",
        "  metadata = [\n",
        "      {'text': x['chunk'],\n",
        "       'source': x['source'],\n",
        "       'title': x['title']} for i, x in batch.iterrows()\n",
        "  ]\n",
        "  # add to pinecone\n",
        "  index.upsert(vectors=zip(ids, embeds, metadata))\n",
        "\n",
        "print(index.describe_index_stats())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfEfzmwVmvqQ",
        "outputId": "2220d402-7ddb-4241-dfb9-c0fa0a886a71"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49/49 [01:50<00:00,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'': {'vector_count': 4838}},\n",
            " 'total_vector_count': 4838}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Augmented Generation\n",
        "Now the vector embedding can be incorporated into the chatbot as a knowledge base. For example, we can query the index about relevant information on Llama 2"
      ],
      "metadata": {
        "id": "ApfWNGbiNdoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")\n",
        "\n",
        "query = \"What is so special about Llama 2?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G40svy98NY9K",
        "outputId": "a6312871-07f9-4c27-8278-fdaa070324f5"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-58d7a0d0e073>:5: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
            "  vectorstore = Pinecone(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will connect the relevant results to the query prompt."
      ],
      "metadata": {
        "id": "OGy8_vklPs02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_prompt(query: str):\n",
        "  # return top 3 results from knowledge base\n",
        "  results = vectorstore.similarity_search(query, k=3)\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "  augmented_prompt = f\"\"\"Using the context below, answer the query.\n",
        "  Contexts:\n",
        "  {source_knowledge}\n",
        "\n",
        "  Query: {query}\"\"\"\n",
        "  return augmented_prompt\n",
        "\n",
        "print(augment_prompt(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViN1QkPEPbJ5",
        "outputId": "69ff6d66-f3b5-47f0-f9e6-cd8bd3594f44"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the context below, answer the query.\n",
            "  Contexts:\n",
            "  Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov Thomas Scialom\u0003\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
            "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
            "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
            "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
            "the community to advance AI alignment research.\n",
            "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
            "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
            "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
            "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
            "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
            "arXiv:2302.13971 , 2023.\n",
            "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
            "and Illia Polosukhin. Attention is all you need, 2017.\n",
            "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
            "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
            "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
            "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
            "\n",
            "  Query: What is so special about Llama 2?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qSJwVKSQxah",
        "outputId": "a3aef783-4c4f-4e38-ffce-8711d36e92a7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Their fine-tuned LLMs, specifically optimized for dialogue use cases, outperform open-source chat models on various benchmarks and have shown promising results in terms of helpfulness and safety based on human evaluations. These models may serve as a viable substitute for closed-source models in certain applications. The development and release of LLama 2 aim to provide models that demonstrate superior performance compared to existing open-source models and are competitive with closed-source models in terms of usability and safety.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we ask about a new prompt chatgpt cannot respond with as much detail. It only remembers what was already said above."
      ],
      "metadata": {
        "id": "es9FZCSpSVvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"what safety measure were used in the development of Llama 2?\"\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI3PBy7IRSWH",
        "outputId": "48f12798-a24f-4227-c219-495fe88020bb"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The development of Llama 2 focused on implementing safety measures to ensure the models were optimized for dialogue use cases and considered safe for deployment. The safety measures used in the development of Llama 2 included humane evaluations for helpfulness and safety as substitutes for closed-source models. Additionally, the fine-tuned LLMs in Llama 2 were designed to perform better than existing open-source models in terms of safety and helpfulness. These evaluations and optimizations were part of the strategy to enhance the usability and safety of the Llama 2 models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to get new relevant source knowledge for the prompt."
      ],
      "metadata": {
        "id": "xn9UAsDdSiTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\"what safety measure were used in the development of Llama 2?\")\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVJFFOecShRp",
        "outputId": "824c619e-5186-46ac-99d2-5c6b9fb60737"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the development of Llama 2, safety measures were taken to increase the safety of the models. These safety measures included using safety-specific data annotation and tuning, conducting red-teaming exercises, and employing iterative evaluations. Additionally, the researchers shared a thorough description of their fine-tuning methodology and their approach to improving LLM safety to encourage more responsible development of large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete index to save resources\n",
        "pc.delete_index(index_name)"
      ],
      "metadata": {
        "id": "avYqz6SXR2Kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}